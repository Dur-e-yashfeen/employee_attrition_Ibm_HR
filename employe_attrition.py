# -*- coding: utf-8 -*-
"""employe_attrition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rJ961E_ieX5wkorXqvs2PlC1qtS2drXJ
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
# %matplotlib inline

# loading the dataset
df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
df.head()# displays the first 5 rows of the dataset

df.drop(columns=['EmployeeCount','EmployeeNumber','Over18','StandardHours'], inplace=True)

# checking the class imblance of attrition
sns.countplot(x=df['Attrition'], palette="coolwarm")
plt.title('Attrition Distribution')
plt.show()

# Select only the numeric columns for correlation
numeric_df = df.select_dtypes(include=[float, int])

# correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation with Attrition')
plt.show()

# Attrition by department
plt.figure(figsize=(8, 5))
sns.countplot(x=df['Department'], hue=df['Attrition'], palette="Set2")
plt.title('Attrition by Department')
plt.xticks(rotation=45)
plt.show()

# Attrition by Age group
plt.figure(figsize=(10, 8))
sns.histplot(df[df["Attrition"] == 1]["Age"], bins=20, kde=True, color="red", label="Leave")
sns.histplot(df[df["Attrition"] == 0]["Age"], bins=20, kde=True, color="green", label="Stay")
plt.title('Attrition by Age')
plt.legend()
plt.show()  # Add parentheses to plt.show

# # encode the cetagorical variables using for loop where object and cetagory data type are given
for col in df.columns:
    if df[col].dtype == 'object' or df[col].dtype == 'category':
        df[col] = LabelEncoder().fit_transform(df[col])

# check the outlier
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(df['Age'])
plt.show()

# x  and y columns
x = df.drop(columns=['Attrition'])
y = df['Attrition']

# train test split the data
x_train,x_test,y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

from sklearn.preprocessing import MinMaxScaler
# create a MinMaxScaler object
scaler = MinMaxScaler()
# fit and transform the data
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# call the model
lr = LogisticRegression(max_iter= 100, random_state=42)
lr.fit(x_train,y_train)
y_pred = lr.predict(x_test)

# evaluate the model
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(f'Recall: {recall_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))
print(f'Precision: {precision_score(y_test, y_pred)}')
print(f'F1 Score: {f1_score(y_test, y_pred)}')
print(f'Classification Report: \n {classification_report(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))

import shap
# SHAP explanation
# Use shap.Explainer or shap.LinearExplainer for Logistic Regression
# shap_explainer = shap.LinearExplainer(lr, x_train) # if you want to use LinearExplainer
shap_explainer = shap.Explainer(lr, x_train) # Use Explainer which automatically selects the best method
shap_values = shap_explainer.shap_values(x_test)

# Select appropriate SHAP values for summary plot
if isinstance(shap_values, list) and len(shap_values) > 1:
    shap_values_to_plot = shap_values[1]  # For binary classification, use SHAP values of class 1
else:
    shap_values_to_plot = shap_values

# Summary plot for class 1
shap.summary_plot(shap_values_to_plot, x_test)